@page "/openApi/productsChatV4"
@rendermode InteractiveServer
@using Microsoft.Extensions.AI
@using Microsoft.Extensions.Options
@using Microsoft.SemanticKernel
@using Microsoft.SemanticKernel.ChatCompletion
@using Microsoft.SemanticKernel.Connectors.OpenAI
@using SemanticAgents.Abstractions
@using SemanticAgents.Client.Abstractions
@using SemanticAgents.Client.Utility
@using SemanticAgents.Configuration
@using SemanticAgents.SemanticKernelDTOs
@using SemanticAgents.SemanticKernelLocalHttpClients
@using Microsoft.SemanticKernel.Plugins.OpenApi
@using System.Text
@implements IDisposable

<PageTitle>Products Chat</PageTitle>

<div class="row">
    <div class="col-6">
        <h4>Query Products API</h4>
    </div>
    @if (!string.IsNullOrWhiteSpace(_textModelId) && !string.IsNullOrWhiteSpace(_imageModelId) && !string.IsNullOrWhiteSpace(_modelProvider))
    {
        <div class="col-6">
            <p class="mx-0 my-0 text-secondary-alt text-sm-end">
                <i class="bi-lightning-charge-fill"></i>
                <b><i>&nbsp; Text Model: @_textModelId &nbsp;  &nbsp; Image Model: @_imageModelId &nbsp;  &nbsp;  Provider: @_modelProvider</i></b>
            </p>
        </div>
    }
    else if (!string.IsNullOrWhiteSpace(_textModelId) && !string.IsNullOrWhiteSpace(_modelProvider))
    {
        <div class="col-6">
            <p class="mx-0 my-0 text-secondary-alt text-sm-end">
                <i class="bi-lightning-charge-fill"></i>
                <b><i>&nbsp; Text Model: @_textModelId &nbsp; &nbsp;  Provider: @_modelProvider</i></b>
            </p>
        </div>
    }
</div>

@if (!string.IsNullOrEmpty(_errorMessage))
{
    <div class="alert alert-danger mt-2" role="alert">
        @_errorMessage
    </div>
    <a class="btn button-custom-success form-control mt-2" @onclick="ClearErrorMessage">Clear</a>
}


<div class="row mt-2">
    <div class="col-2">
        <label class="text-sm-start mt-1">Model Provider:</label>
        <select class="bg-dark text-light form-select form-select-sm mt-1" @bind="_modelProvider" @bind:after="OnModelProviderChanged">
            <option value="OpenAI">OpenAI</option>
            <option value="Grok">Grok</option>
            <option value="LMStudio">LMStudio</option>
            <option value="Ollama">Ollama</option>
        </select>
    </div>
    <div class="col-3">
        <label class="text-sm-start mt-1">Text Model:</label>
        @if (_modelProvider == "OpenAI" || _modelProvider == "Grok")
        {
            <select class="bg-dark text-light form-select form-select-sm mt-1" @bind="_textModelId">
                @if (_textModelList is not null)
                {
                    @foreach (var model in _textModelList)
                    {
                        <option value="@model">@model</option>
                    }
                }
            </select>
        }
        else
        {
            <input type="text" class="bg-dark text-light form-control form-control-sm mt-1" @bind="_textModelId" readonly />
        }
    </div>
    <div class="col-3">
        <label class="text-sm-start mt-1">Image Model:</label>
        @if (_modelProvider == "OpenAI" || _modelProvider == "Grok")
        {
            <select class="bg-dark text-light form-select form-select-sm mt-1" @bind="_imageModelId">
                @if (_imageModelList is not null)
                {
                    @foreach (var model in _imageModelList)
                    {
                        <option value="@model">@model</option>
                    }
                }
            </select>
        }
        else
        {
            <input type="text" class="bg-dark text-light form-control form-control-sm mt-1" @bind="_imageModelId" readonly />
        }
    </div>
    @if (_modelConfigurationIsInvalid)
    {
        <div class="col-2">
            <a class="btn button-custom-warning form-control mt-4" @onclick="OnModelProviderChanged">Retry</a>
        </div>
    }
    @if (_lmStudioModelDTO is not null)
    {
        <div class="col-2">
            <a class="btn button-custom-yellow form-control mt-4" @onclick="ShowLoadedLMStudioModel">Model Info</a>
        </div>
    }
</div>

<div class="row mt-2">
    @if (_kernel is null)
    {
        <div class="col-2">
            <a class="btn button-custom-success form-control" @onclick="ConfigureKernelAsync">Configure Kernel</a>
        </div>
    }
    else if (_modelConfigurationChanged)
    {
        <div class="col-2">
            <a class="btn button-custom-warning form-control" @onclick="ConfigureKernelAsync">Reconfigure Kernel</a>
        </div>
    }
    @if (_chatHistory is not null && _chatHistory.Count > 1)
    {
        <div class="col-2">
            <a class="btn button-custom-primary form-control" @onclick="InitailizeChatHistory">Reset Chat History</a>
        </div>

        <div class="col-2">
            <a class="btn button-custom-warning form-control" @onclick="CheckIfCancellationTokenExists">Check CTS</a>
        </div>
        <div class="col-1 text-end">
            <p class="mt-2">History:</p>
        </div>
        <div class="col-1">
            <input type="number" class="bg-dark text-light form-control mt-1" @bind-value="_chatHistoryCount" />
        </div>
        @if (_chatHistory.Count >= _reductionThreshold)
        {
            <div class="col-2">
                <a class="btn button-custom-warning form-control" @onclick="ReduceChatHistory">Reduce</a>
            </div>

            <div class="col-2 text-center">
                <select class="bg-dark text-light form-select mt-1" @bind="_reductionType">
                    @foreach (var type in _reductionTypesList)
                    {
                        <option value=@type>@type</option>
                    }
                </select>
            </div>
        }
        else
        {
            <div class="col-2">
            </div>
        }
    }
</div>

@if (_chatHistory is not null)
{
    <div class="row">
        <div class="col-8">
            @* LEFT COLUMN - CHAT HISTORY *@
            <div class="row mt-2">
                <label for="userMessage" class="form-label">Your Message:</label>
                <textarea rows="4" class="bg-dark text-light form-control" id="userMessage"
                          @bind="_userMessage" @bind:after="SubmitUserMessage" @onfocus="ClearUserMessage" />
            </div>
            <div class="row mt-2">
                <div class="col-4 offset-4">
                    @if (_processingUserRequest)
                    {
                        <btn class="btn button-custom-danger form-control" @onclick="CancelRequest">Cancel</btn>
                    }
                    else
                    {
                        <btn class="btn button-custom-success form-control" @onclick="SubmitUserMessage">Submit</btn>
                    }
                </div>
            </div>
            @if (_showChatHistory)
            {
                <div class="row mt-2">
                    <label for="historyList" class="form-label">History:</label>
                    <textarea rows="9" class="bg-dark text-light form-control" id="historyList" @bind="_messageHistoryList" />
                </div>
            }
            else
            {
                <div class="row mt-2">
                    <label for="assistantMessage" class="form-label">Assistant Message:</label>
                    <textarea rows="9" class="bg-dark text-light form-control" id="assistantMessage" @bind="_assistantMessage" />
                </div>
            }
            <div class="row mt-2">
                <div class="col-4 offset-4">
                    <btn class="btn button-custom-success form-control" @onclick="ToggleShowChatHistory">@_showChatHistoryButtonText</btn>
                </div>
            </div>
        </div>
        <div class="col-3 offset-1">
            @* RIGHT COLUMN - PROMPT EXECUTION SETTINGS *@

            @if (_promptExecutionSettings is not null)
            {
                <br />
                <h5 class="text-info-alt mt-2">Prompt Execution Settings:</h5>
                <div class="row">
                    <div class="col-6">
                        <label class="label" for="maxtokens">Max Tokens:</label>
                    </div>
                    <div class="col-6">
                        <input type="number" class="form-control bg-dark text-light " id="maxtokens"
                               @bind-value="@_promptExecutionSettings.MaxTokens" />
                    </div>
                </div>
                <div class="row mt-2">
                    <div class="col-6">
                        <label class="label" for="useTopP">Use Top P:</label>
                    </div>
                    <div class="col-6">
                        <InputCheckbox class="form-check-input bg-dark text-light " id="useTopP" @bind-value="@_useTopP" />
                    </div>
                </div>
                @if (_useTopP)
                {
                    <div class="row mt-2">
                        <div class="col-6">
                            <label class="label" for="topp">Top P:</label>
                        </div>
                        <div class="col-6">
                            <input type="number" class="form-control bg-dark text-light " id="topp"
                                   @bind-value="@_promptExecutionSettings.TopP" />
                        </div>
                    </div>
                }
                else
                {
                    <div class="row mt-2">
                        <div class="col-6">
                            <label class="label" for="temp">Temperature:</label>
                        </div>
                        <div class="col-6">
                            <input type="number" class="form-control bg-dark text-light " id="temp"
                                   @bind-value="@_promptExecutionSettings.Temperature" />
                        </div>
                    </div>
                }
                <div class="row mt-2">
                    <div class="col-6">
                        <label class="label" for="freqpen">Frequency Penalty:</label>
                    </div>
                    <div class="col-6">
                        <input type="number" class="form-control bg-dark text-light " id="freqpen"
                               @bind-value="@_promptExecutionSettings.FrequencyPenalty" />
                    </div>
                </div>
                <div class="row mt-2">
                    <div class="col-6">
                        <label class="label" for="prespen">Presence Penalty:</label>
                    </div>
                    <div class="col-6">
                        <input type="number" class="form-control bg-dark text-light " id="prespen"
                               @bind-value="@_promptExecutionSettings.PresencePenalty" />
                    </div>
                </div>
                <div class="row mt-2">
                    <div class="col-6">
                        <label class="label" for="tool">Tool Call Behavior:</label>
                    </div>
                    <div class="col-6">
                        <select type="text" class="form-select bg-dark text-light " id="tool"
                                @bind="@_toolCallBehaviorString" @bind:after="OnToolCallBehaviorStringChange">
                            <option value="Auto Invoke">Auto Invoke</option>
                            <option value="Enable">Enable</option>
                        </select>

                    </div>
                </div>
                @if (_thinking)
                {
                    <br />
                    <br />
                    <div class="row">
                        <div class="col-12 text-center">
                            <p class="text-yellow-alt"><i>... processing request ...</i></p>
                        </div>
                    </div>
                    <div class="mx-auto text-center mb-3 mt-3">
                        <div class="spinner-border text-yellow-alt role-status">
                            <span class="visually-hidden">Loading...</span>
                        </div>
                    </div>
                }
                @if (_loadingConfiguration)
                {
                    <div class="row mt-2">
                        <div class="col-12">
                            <p class="text-yellow-alt"><i>... loading configuration ...</i></p>
                        </div>
                    </div>
                    <div class="mx-auto text-center mb-3 mt-3" style="height:160px;">
                        <div class="spinner-border text-yellow-alt role-status">
                            <span class="visually-hidden">Loading...</span>
                        </div>
                    </div>
                }
            }
        </div>
    </div>
}

@if (_showLMStudioModel && _lmStudioModelDTO is not null)
{
    <LoadedModelModal ModelDTO="_lmStudioModelDTO" ModalTitle="Loaded Model" Capabilities="@_loadedModelCapabilities"
                      Cancel="HideLoadedLMStudioModel" CancelButtonTitle="Close" />
}

@code {

    private Kernel? _kernel { get; set; } = default!;
    [Inject]
    private IOptions<SemanticKernelSettings> _semanticKernelSettings { get; set; } = default!;
    [Inject]
    private IToastrService _toastr { get; set; } = default!;
    [Inject]
    private ILMStudioHttpProvider _lmStudioHttpProvider { get; set; } = default!;
    [Inject]
    private IOllamaHttpProvider _ollamaHttpProvider { get; set; } = default!;

    private async Task OnModelProviderChanged()
    {
        _loadingConfiguration = true;

        switch (_modelProvider)
        {
            case "OpenAI":
                _lmStudioModelDTO = null;
                _ollamaModelDTO = null;
                _textModelId = StaticData.OpenAI_TextModel_gpt3_5_Turbo;                    // _semanticKernelSettings?.Value.OpenAI_TextModelId;
                _textModelList = StaticData.OpenAI_TextModels;
                _imageModelId = StaticData.OpenAI_ImageModel_dall_e_3;                      //_semanticKernelSettings?.Value.OpenAI_ImageModelId;
                _imageModelList = StaticData.OpenAI_ImageModels;
                _modelProviderEndpoint = null;
                _apiKey = _semanticKernelSettings?.Value.OpenAI_ApiKey;
                break;
            case "Grok":
                _lmStudioModelDTO = null;
                _ollamaModelDTO = null;
                _textModelId = StaticData.Grok_TextModel_Grok_3;                    // _semanticKernelSettings?.Value.Grok_TextModelId;
                _textModelList = StaticData.Grok_TextModels;
                _imageModelId = StaticData.Grok_ImageModel_Grok_2_Image_1212;       // _semanticKernelSettings?.Value.Grok_ImageModelId;
                _imageModelList = StaticData.Grok_ImageModels;
                _modelProviderEndpoint = StaticData.Grok_Endpoint;                  //_semanticKernelSettings?.Value.Grok_Endpoint;
                _apiKey = _semanticKernelSettings?.Value.Grok_ApiKey;
                break;
            case "LMStudio":
                _lmStudioModelDTO = null;
                _ollamaModelDTO = null;
                _modelProviderEndpoint = StaticData.LMStudio_Endpoint;              // _semanticKernelSettings?.Value.LMStudio_Endpoint;
                _apiKey = null;
                _textModelId = await GetLMStudioStateAsync();
                _imageModelId = null;
                break;
            case "Ollama":
                _lmStudioModelDTO = null;
                _ollamaModelDTO = null;
                _modelProviderEndpoint = StaticData.Ollama_Endpoint;              // _semanticKernelSettings?.Value.Ollama_Endpoint;
                _apiKey = null;
                _textModelId = await GetOllamaStateAsync();
                _imageModelId = null;
                break;
            default:
                _modelProviderEndpoint = null;
                _apiKey = null;
                _textModelId = null;
                _textModelList = null;
                _imageModelId = null;
                _imageModelList = null;
                break;
        }

        if (string.IsNullOrWhiteSpace(_textModelId))
        {
            _modelConfigurationIsInvalid = true;
        }
        else
        {
            _modelConfigurationIsInvalid = false;
            _errorMessage = null;
        }
        _loadingConfiguration = false;
        if (_kernel is not null) _modelConfigurationChanged = true;
    }

    private async Task ConfigureKernelAsync()
    {
        _loadingConfiguration = true;

        if (_modelProvider == "OpenAI" || _modelProvider == "Grok")
        {
            _kernel = Kernel.CreateBuilder()
                .AddOpenAIChatCompletion(
                    modelId: _textModelId!,
                    apiKey: _apiKey!,
                    endpoint: string.IsNullOrWhiteSpace(_modelProviderEndpoint) ? null! : new Uri(_modelProviderEndpoint!))
                .AddOpenAITextToImage(
                    apiKey: _apiKey!,
                    modelId: _imageModelId!)
                .Build();
        }
        else if (_modelProvider == "LMStudio" || _modelProvider == "Ollama")
        {
            _kernel = Kernel.CreateBuilder()
                .AddOpenAIChatCompletion(
                    modelId: _textModelId!,
                    apiKey: _apiKey!,
                    endpoint: string.IsNullOrWhiteSpace(_modelProviderEndpoint) ? null! : new Uri(_modelProviderEndpoint!))
                .Build();
        }
        else
        {
            _errorMessage = "Invalid Model Provider.";
            _modelConfigurationIsInvalid = true;
            return;
        }

        await ConfigureKernelPluginsAsync();
        if (_chatHistory is null) InitailizeChatHistory();

        _loadingConfiguration = false;
        _modelConfigurationChanged = false;
        await _toastr.ShowToastrSuccess($"Chat instantiated with a Kernel configured using {_modelProvider}.");
    }

    // Model state information
    #region Model State Information
    private bool _modelConfigurationChanged;    // set to true if kernel is already configured - and selected model provider or selected text or image model changes
    private bool _modelConfigurationIsInvalid;
    private string? _modelProvider;
    private string? _modelProviderEndpoint;
    private string? _apiKey;
    private string? _textModelId;
    private List<string>? _textModelList;
    private string? _imageModelId;
    private List<string>? _imageModelList;
    private LMStudioModelDTO? _lmStudioModelDTO;
    private OllamaModelDTO? _ollamaModelDTO;
    private string? _loadedModelCapabilities;
    private bool _showLMStudioModel;
    private void ShowLoadedLMStudioModel()
    {
        if (_lmStudioModelDTO is null) return;
        // await Toastr.ShowToastrSuccess($"The loaded model is NOT null!!!");
        _showLMStudioModel = true;
    }
    private void HideLoadedLMStudioModel()
    {
        _showLMStudioModel = false;
    }
    #endregion Model State Information

    // User message handling and information
    #region Messages and Information
    private bool _thinking = false;
    private bool _loadingConfiguration = false;
    private bool _processingUserRequest = false;
    private string? _userMessage;
    private string? _errorMessage;
    private void ClearUserMessage() => _userMessage = string.Empty;
    private void ClearErrorMessage() => _errorMessage = null;
    #endregion Messages and Information

    // Chat parameters
    #region Chat Parameters
    private IChatCompletionService? _chatService;
    private ChatHistory? _chatHistory;
    private string? _assistantMessage = string.Empty;
    private string? _messageHistoryList;
    private int _chatHistoryCount = 0;
    private bool _showReductionOption;
    private int _reductionThreshold = 6;
    private bool _showChatHistory;
    private string _showChatHistoryButtonText = "Show Chat History";
    private void ToggleShowChatHistory()
    {
        _showChatHistory = !_showChatHistory;
        if (_showChatHistory) _showChatHistoryButtonText = "Hide Chat History";
        else _showChatHistoryButtonText = "Show Chat History";
    }
    private bool _useTopP = false;
    private OpenAIPromptExecutionSettings? _promptExecutionSettings = new()
    {
        MaxTokens = 4000,
        Temperature = 0.7,
        TopP = 0.95,
        FrequencyPenalty = 0,
        PresencePenalty = 0,
        ToolCallBehavior = ToolCallBehavior.AutoInvokeKernelFunctions
    };
    private string _toolCallBehaviorString = "Auto Invoke";
    private void OnToolCallBehaviorStringChange()
    {
        if (_promptExecutionSettings is null)
        {
            _errorMessage = $"Prompt Execution Settings are not configured.";
            return;
        }
        if (_toolCallBehaviorString == "Auto Invoke") _promptExecutionSettings.ToolCallBehavior = ToolCallBehavior.AutoInvokeKernelFunctions;
        else if (_toolCallBehaviorString == "Enable") _promptExecutionSettings.ToolCallBehavior = ToolCallBehavior.EnableKernelFunctions;
    }
    #endregion Chat Parameters

    // Cancellation token handling
    #region Cancellation Token Handling
    private CancellationTokenSource? _cancellationTokenSource;
    private async Task CancelRequest()
    {
        if (_cancellationTokenSource is null) return;
        await _cancellationTokenSource.CancelAsync();
        _cancellationTokenSource.Dispose();
    }
    private async Task CheckIfCancellationTokenExists()
    {
        if (_cancellationTokenSource is null) await _toastr.ShowToastrInfo($"The CancellationTokenSource IS NULL.");
        else await _toastr.ShowToastrInfo($"The CancellationTokenSource EXISTS.");
    }
    #endregion Cancellation Token Handling

    #region Configuring Kernel and Chat
    private async Task<string?> GetLMStudioStateAsync()
    {
        if (_lmStudioHttpProvider is null)
        {
            _errorMessage = $"The LMStudio Http Provider is null";
            return null;
        }

        var result = await _lmStudioHttpProvider.GetLMStudioLoadedModelsAsync();
        if (result.IsSuccess)
        {
            List<LMStudioModelDTO>? loadedModels = result.LoadedModels?.ToList();
            if (loadedModels is not null && loadedModels.Any())
            {
                _lmStudioModelDTO = loadedModels.FirstOrDefault(m => m.State == "loaded");
                if (_lmStudioModelDTO is not null)
                {
                    if (_lmStudioModelDTO.Capabilities is not null) _loadedModelCapabilities = string.Join(", ", _lmStudioModelDTO.Capabilities);
                    return _lmStudioModelDTO.Id;
                }
                else
                {
                    _errorMessage = "Please check LM Studio configuration and try again. A text model was not loaded.";
                }
            }
        }
        else _errorMessage = result.ErrorMessage;

        // _thinking = false;
        return null;
    }

    private async Task<string?> GetOllamaStateAsync()
    {
        // _thinking = true;

        bool success = false;
        int retryCount = 3;
        IEnumerable<OllamaModelDTO>? loadedModels = null;
        string? errorMessage = null;

        while (!success && retryCount > 0)
        {
            (bool IsSuccess, IEnumerable<OllamaModelDTO>? LoadedModels, string? ErrorMessage) result = await _ollamaHttpProvider.GetOllamaLoadedModelsAsync();

            success = result.IsSuccess;
            loadedModels = result.LoadedModels;
            errorMessage = result.ErrorMessage;

            retryCount--;
        }

        if (success)
        {
            if (loadedModels is not null && loadedModels.Any())
            {
                OllamaModelDTO? loaded = loadedModels.FirstOrDefault();
                if (loaded is not null)
                {
                    return loaded.Model;
                }
                else
                {
                    errorMessage = $"A running Ollam model was not found. Please check Ollama configuration. " +
                    "Note that it may take a few attemps for the model query to respond.";
                }
            }
        }
        else _errorMessage = errorMessage;

        // _thinking = false;
        return null;
    }

    private async Task ConfigureKernelPluginsAsync()
    {
        if (_kernel is not null)
        {
            await _kernel.ImportPluginFromOpenApiAsync(
                pluginName: "products",
                uri: new Uri("https://localhost:7152/openapi/v1.json"),
                executionParameters: new OpenApiFunctionExecutionParameters() { EnablePayloadNamespacing = true }
            );

            // _kernel.ImportPluginFromType<TimePlugin>();
        }
    }

    private void InitailizeChatHistory()
    {
        _chatHistory = new ChatHistory();   // just using Clear() not working well.
        _chatHistory.AddSystemMessage("You are a helpful assistant who has access to a collection of ProductDTOs using the plugin " +
            "with the pluginName \'products\'. ProductDTOs are equevalent to Products.  Any reference to the term Products in user messages " +
            "or Function descriptions should be considered to refer to ProductDTOs. Any reference to an Image should be understood to refer " +
            "to an ImageDataDTO.  Any reference to a Document should be understood to refer to a DocumentDataDTO.");
        _userMessage = string.Empty;
        _assistantMessage = string.Empty;
        _messageHistoryList = string.Empty;
        _chatHistoryCount = _chatHistory.Count;
    }
    #endregion Configuring Kernel and Chat

    private void LogKernelConfiguration()
    {
        StringBuilder stringBuilder = new StringBuilder($"\nREQUEST SUBMISSION KERNEL CONFIGURATION: \n");
        stringBuilder.AppendLine($"    Model Provider: {_modelProvider}");
        stringBuilder.AppendLine($"    Model Provider Endpoint: {_modelProviderEndpoint}");
        stringBuilder.AppendLine($"    API Key: {_apiKey?.Substring(0, 6)}...");
        stringBuilder.AppendLine($"    Text Model Id: {_textModelId}");
        stringBuilder.AppendLine($"    Image Model Id: {_imageModelId}\n");
        Console.WriteLine(stringBuilder.ToString());
    }

    #region Message Submission and Handling
    private async Task SubmitUserMessage()
    {
        LogKernelConfiguration();

        try
        {
            if (string.IsNullOrWhiteSpace(_userMessage)) { _errorMessage = "Please enter a message."; return; }

            ValidatePromptExecutionSettings();
            if (!string.IsNullOrEmpty(_errorMessage)) return;

            using (_cancellationTokenSource = new CancellationTokenSource())
            {
                _thinking = true;
                _processingUserRequest = true;
                _chatHistory!.AddUserMessage(_userMessage);
                _messageHistoryList += $"USER: {_userMessage}\n\n";
                _assistantMessage = string.Empty;

                _chatService = _kernel?.GetRequiredService<IChatCompletionService>();

                // // OPTION 1: NON-STREAMING
                // ChatMessageContent assistantContent = await _chatService!.GetChatMessageContentAsync(_chatHistory!, _promptExecutionSettings, _kernel);
                // _messageHistoryList += ($"{assistantContent.Content}\n\n");
                // if (assistantContent.Content is not null) _chatHistory.AddAssistantMessage(assistantContent.Content);
                // _userMessage = string.Empty;

                // OPTION 2: STREAMING
                await foreach (StreamingChatMessageContent chunk in _chatService!.GetStreamingChatMessageContentsAsync(
                    _chatHistory, _promptExecutionSettings, _kernel, _cancellationTokenSource.Token))
                {
                    _assistantMessage += chunk.Content;
                    if (_cancellationTokenSource.Token.IsCancellationRequested)
                    {
                        Console.WriteLine("Breaking out of streaming loop - the Operation has been cancelled by user.");
                        _assistantMessage += $"\nOperation cancelled by user.";
                        break;
                    }
                }

                _messageHistoryList += ($"ASSISTANT: {_assistantMessage}\n\n");
                if (_assistantMessage is not null) _chatHistory.AddAssistantMessage(_assistantMessage);
                // _userMessage = string.Empty; // Note: _userMessage is cleared with 'onfocus' event on the html input textarea element
                _chatHistoryCount = _chatHistory.Count;
                if (_chatHistory.Count >= _reductionThreshold) _showReductionOption = true;
                _thinking = false;
                _processingUserRequest = false;
            }
        }
        catch (OperationCanceledException)
        {
            await _toastr.ShowToastrInfo($"OPERATION CANCELLED EXCEPTION: Operation cancelled by user.");
            Console.WriteLine("Operation Canceled Exception: Operation cancelled by user.");
            _assistantMessage += $"\nOperation cancelled by user.";
            _thinking = false;
            _processingUserRequest = false;
        }
        catch (System.ClientModel.ClientResultException crex)
        {
            await _toastr.ShowToastrError($"CLIENT RESULT EXCEPTION: {crex.Message}");
            Console.WriteLine($"****** \nEXCEPTION: {crex.Message}\n******");
            _errorMessage = $"{crex.Message}. Please ensure the model provider endpoint is correct.";
            _thinking = false;
            _processingUserRequest = false;
        }
        catch (Exception ex)
        {
            await _toastr.ShowToastrError($"EXCEPTION: {ex.Message}");
            Console.WriteLine($"****** \nEXCEPTION: TYPE: {ex.GetType().FullName}. \nMESSAGE: {ex.Message}\n******");
            _errorMessage = ex.Message;
            _thinking = false;
            _processingUserRequest = false;
        }
    }

    // Supporting methods for SubmitUserMessage()
    private void ValidatePromptExecutionSettings()
    {
        if (_promptExecutionSettings is null) _errorMessage = $"Prompt Execution Settings are not accessible.";
        else
        {
            if (_promptExecutionSettings.MaxTokens <= 0) _promptExecutionSettings.MaxTokens = 4000;
            if (_promptExecutionSettings.FrequencyPenalty < -2.0) _promptExecutionSettings.FrequencyPenalty = -2.0;
            if (_promptExecutionSettings.FrequencyPenalty > 2.0) _promptExecutionSettings.FrequencyPenalty = 2.0;
            if (_promptExecutionSettings.PresencePenalty < -2.0) _promptExecutionSettings.PresencePenalty = -2.0;
            if (_promptExecutionSettings.PresencePenalty > 2.0) _promptExecutionSettings.PresencePenalty = 2.0;
            if (_promptExecutionSettings.Temperature < 0) _promptExecutionSettings.Temperature = 0;
            if (_promptExecutionSettings.Temperature > 1.0) _promptExecutionSettings.Temperature = 1.0;
            if (_promptExecutionSettings.TopP < 0) _promptExecutionSettings.TopP = 0;
            if (_promptExecutionSettings.TopP > 1.0) _promptExecutionSettings.TopP = 1.0;
        }
    }
    #endregion Message Submission and Handling

    // Chat History Reduction
    #region Chat History Reduction
    // Note: _messageHistoryList does not include the system message while _chatHistory does
    private List<string> _reductionTypesList = new List<string>() { "truncation", "summarization" };
    private string _reductionType = "truncation";
    private async Task ReduceChatHistory()
    {
        Console.WriteLine($"ReduceChatHistory called with reduction type {_reductionType}");

        try
        {
            if (_chatHistory is null)
            {
                _errorMessage = $"The ChatHistory is null.";
                return;
            }

            _thinking = true;

            int initialChatCount = _chatHistory.Count;

            IChatHistoryReducer? reducer = null;
            if (_reductionType == "truncation")
            {
                reducer = new ChatHistoryTruncationReducer(2);
            }
            else if (_reductionType == "summarization")
            {
                if (_chatService is null)
                {
                    _errorMessage = $"Cannot use Summarization to reduce chat history as the ChatService is null.";
                    return;
                }
                reducer = new ChatHistorySummarizationReducer(_chatService, 2);
            }

            if (reducer is null)
            {
                _errorMessage = $"A ChatHistory Reducer was not properly configured.";
                return;
            }
            IEnumerable<ChatMessageContent>? reducedHistory = await reducer!.ReduceAsync(_chatHistory);
            if (reducedHistory is not null)
            {
                _chatHistory = new ChatHistory(reducedHistory);
                _showReductionOption = false;
            }
            _thinking = false;
            _chatHistoryCount = _chatHistory.Count;
            _messageHistoryList = string.Empty;
            foreach (var message in _chatHistory)
            {
                if (message.Role == AuthorRole.User) _messageHistoryList += ($"USER: {message.Content}\n\n");
                else if (message.Role == AuthorRole.Assistant) _messageHistoryList += ($"ASSISTANT: {_assistantMessage}\n\n");
            }

            _showReductionOption = false;
            await _toastr.ShowToastrSuccess($"Chat history reduced from {initialChatCount} messages to {_chatHistory.Count} " +
            $"messages using {_reductionType} reduction.");

        }
        catch (Exception ex)
        {
            Console.WriteLine($"****** \nEXCEPTION: {ex.Message}\n******");
            _errorMessage = ex.Message;
            _thinking = false;
        }
    }
    #endregion Chat History Reduction

    protected override void OnInitialized()
    {
        _modelProvider = "OpenAI";
        _textModelId = StaticData.OpenAI_TextModel_gpt3_5_Turbo;
        _textModelList = StaticData.OpenAI_TextModels;
        _imageModelId = StaticData.OpenAI_ImageModel_dall_e_3;
        _imageModelList = StaticData.OpenAI_ImageModels;
        _modelProviderEndpoint = null;
        _apiKey = _semanticKernelSettings?.Value.OpenAI_ApiKey;
    }

    public void Dispose()
    {
        if (_cancellationTokenSource is not null) _cancellationTokenSource.Dispose();
    }
}
